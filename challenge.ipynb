{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f7bcf9e",
   "metadata": {
    "id": "6f7bcf9e"
   },
   "source": [
    "# CS599 Applied Machine Learning - Fall 2022: Class Challenge\n",
    "\n",
    "In this class challenge assignment, you will be building a machine learning model to predict the price of an Airbnb rental given the dataset we have provided. **Total points: 100 pts**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1a309c",
   "metadata": {
    "id": "ac1a309c"
   },
   "source": [
    "To submit your solution, you need to submit a **python (.py)** file, named **challenge.py** on Gradescope. \n",
    "\n",
    "Submission link: https://www.gradescope.com/courses/427800/assignments/2437846\n",
    "\n",
    "<br>\n",
    "\n",
    "There will be a Leaderboard for the challenge, which you can find at https://www.gradescope.com/courses/427800/assignments/2437846/leaderboard\n",
    "\n",
    "You can use a nickname or make it anonymous for the leaderboard shown in the red box as below\n",
    "<img src=\"https://raw.githubusercontent.com/chaudatascience/cs599_fall2022/master/challenge/submit.png\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5dd172",
   "metadata": {
    "id": "9f5dd172"
   },
   "source": [
    "To encourage you to get started early on the challenge, you are required to submit an **initial submission** by midterm (due on **Nov 29, 11:59 pm**). For this submission, your model needs to be better than the linear model with random weights that we provided. The **final submission** will be due on **Dec 8, 11:59 pm**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cf921c",
   "metadata": {
    "id": "27cf921c"
   },
   "source": [
    "## Problem and dataset description\n",
    "Pricing a rental property such as an apartment or house on Airbnb is a difficult challenge. A model that accurately predicts the price can potentially help renters and hosts on the platform make better decisions. In this assignment, your task is to train a model that takes features of a listing as input and predicts the price.\n",
    " \n",
    "We have provided you with a dataset collected from the Airbnb website for New York, which has a total of 29,985 entries, each with 764 features. You may use the provided data as you wish in development. We will train your submitted code on the same provided dataset, and will evaluate it on 2 other test sets (one public, and one hidden during the challenge).\n",
    " \n",
    "We have already done some minimal data cleaning for you, such as converting text fields into categorical values and getting rid of the NaN values. To convert text fields into categorical values, we used different strategies depending on the field. For example, sentiment analysis was applied to convert user reviews to numerical values ('comments' column). We added different columns for state names, '1' indicating the location of the property. Column names are included in the data files and are mostly descriptive.\n",
    " \n",
    "Also in this data cleaning step, the price value that we are trying to predict is calculated by taking the log of original price. Hence, the minimum value for our output price is around 2.302 and maximum value is around 9.21 on the training set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90de1040",
   "metadata": {
    "id": "90de1040"
   },
   "source": [
    "## Datasets and Codebase\n",
    "\n",
    "Please download the zip file from the link posted on Piazza/Resources. \n",
    "In this notebook, we implemented a linear regression model with random weights (**attached in the end**). For datasets, there’re 2 CSV files for features and labels:\n",
    "\n",
    "    challenge.ipynb (This file: you need to add your code in here, convert it to .py to submit)\n",
    "    data_cleaned_train_comments_X.csv\n",
    "    data_cleaned_train_y.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2975d36d",
   "metadata": {
    "id": "2975d36d"
   },
   "source": [
    "## Instructions to build your model\n",
    "1.  Implement your model in **challenge.ipynb**. You need to modify the *train()* and *predict()* methods of **Model** class (*attached at the end of this notebook*). You can also add other methods/attributes  to the class, or even add new classes in the same file if needed, but do NOT change the signatures of the *train()* and *predict()* as we will call these 2 methods for evaluating your model.\n",
    "\n",
    "2. To submit, you need to convert your notebook (.ipynb) to a python **(.py)** file. Make sure in the python file, it has a class named **Model**, and in the class, there are two methods: *train* and *predict*. Other experimental code should be removed if needed to avoid time limit exceeded on gradescope.\n",
    " \n",
    "3.  You can submit your code on gradescope to test your model. You can submit as many times you like. The last submission will count as the final model.\n",
    " \n",
    "An example linear regression model with random weights is provided to you in this notebook. Please take a look and replace the code with your own.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ceafaf4",
   "metadata": {
    "id": "0ceafaf4"
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "We will evaluate your model as follows\n",
    "\n",
    "    model = Model() # Model class imported from your submission\n",
    "    X_train = pd.read_csv(\"data_cleaned_train_comments_X.csv\")  # pandas Dataframe\n",
    "    y_train = pd.read_csv(\"data_cleaned_y.csv\")  # pandas Dataframe\n",
    "    model.train(X_train, y_train) # train your model on the dataset provided to you\n",
    "    y_pred = model.predict(X_test) # test your model on the hidden test set (pandas Dataframe)\n",
    "    mse = mean_squared_error(y_test, y_pred) # compute mean squared error\n",
    "\n",
    "There will be 2 test sets, one is public which means you can see MSE on this test set on the Leaderboard (denoted as *MSE (PUBLIC TESTSET)*), and the other one is hidden during the challenge (denoted as *MSE (HIDDEN TESTSET)*). \n",
    "Your score on the hidden test set will be your performance measure. So, don’t try to overfit your model on the public test set. Your final grade will depend on the following criteria:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fed96b",
   "metadata": {
    "id": "21fed96b"
   },
   "source": [
    "1.  \tIs it original code (implemented by you)?\n",
    "2.  \tDoes it take a reasonable time to complete?\n",
    "Your model needs to finish running in under 40 minutes on our machine. We run the code on a machine with 4 CPUs, 6.0GB RAM.\n",
    "3.  \tDoes it achieve a reasonable MSE?\n",
    "    - **Initial submission (10 pts)**: Your model has to be better than the random weights linear model (denoted as RANDOM on Leaderboard) provided in the file. Note this will due on **Nov 29, 11:59pm**.\n",
    "    - **Final submission (90 pts)**: Your last submission will count as the final submission. If its performance is better than our baseline model (denoted as BASELINE on the leaderboard), you will get 60 points for the final submission. If its performance is better than our full credit model (denoted as FULL CREDIT), you will get 90 points for the final submission. Submissions lie in between BASELINE and FULL CREDIT will get 60 points plus some partial credit depending on the performance. We will use MSE on the hidden test set to evaluate your model (lower is better). Due date: **Dec 8, 11:59pm**.\n",
    "![alt text](https://raw.githubusercontent.com/chaudatascience/cs599_fall2022/master/challenge/leaderboard.png)\n",
    "\n",
    "\n",
    "**Bonus**: **Top 3** with the best MSE on the hidden test set will get a 5 point bonus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21b02dd",
   "metadata": {
    "id": "d21b02dd"
   },
   "source": [
    "**Note 1: This is a regression problem** in which we want to predict the price for an AirBnB property. You should try different models and finetune their hyper parameters.  A little feature engineering can also help to boost the performance.\n",
    "\n",
    "**Note 2**: You may NOT use additional datasets. This assignment is meant to challenge you to build a better model, not collect more training data, so please only use the data we provided. We tested the code on Python 3.10 and 3.9, thus it’s highly recommended to use these Python versions for the challenge.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0423bd7",
   "metadata": {
    "id": "d0423bd7"
   },
   "source": [
    "In this challenge, you can only use built-in python modules, and these following:\n",
    "- Numpy\n",
    "- pandas\n",
    "- scikit_learn\n",
    "- matplotlib\n",
    "- scipy\n",
    "- torchsummary\n",
    "- xgboost\n",
    "- torchmetrics\n",
    "- lightgbm\n",
    "- catboost\n",
    "- torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49f69c67",
   "metadata": {
    "id": "49f69c67"
   },
   "outputs": [],
   "source": [
    "### Sample code for the challenge\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.gaussian_process.kernels import WhiteKernel, Matern, RBF, DotProduct, RationalQuadratic, ExpSineSquared\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "class Model:\n",
    "    # Modify your model, default is a linear regression model with random weights\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.to_drop = None\n",
    "        self.model_2 = None\n",
    "        self.model_3 = None\n",
    "\n",
    "    def train(self, X_train: pd.DataFrame, y_train: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Train model with training data.\n",
    "        Currently, we use a linear regression with random weights\n",
    "        You need to modify this function.\n",
    "        :param X_train: shape (N,d)\n",
    "        :param y_train: shape (N,1)\n",
    "            where N is the number of observations, d is feature dimension\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        cor_matrix = X_train.corr().abs()\n",
    "        upper_tri = cor_matrix.where(np.triu(np.ones(cor_matrix.shape),k=1).astype(np.bool))\n",
    "        to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.6)]\n",
    "        self.to_drop = to_drop\n",
    "        X_train = X_train.drop(columns = to_drop)\n",
    "        \n",
    "        \n",
    "#         feature_names = list(X_train.columns)\n",
    "#         for feature in feature_names:\n",
    "#             upper_limit = X_train[feature].mean() + 3*X_train[feature].std()\n",
    "#             lower_limit = X_train[feature].mean() - 3*X_train[feature].std()\n",
    "#             new_X_train = X_train.copy()\n",
    "#             new_X_train.loc[new_X_train[feature]>upper_limit, feature] = upper_limit\n",
    "#             new_X_train.loc[new_X_train[feature]<lower_limit, feature] = lower_limit\n",
    "#             X_train = new_X_train\n",
    "        \n",
    "        y_train = np.array(y_train)\n",
    "        r,c = y_train.shape\n",
    "        y_train = y_train.reshape(r)\n",
    "        \n",
    "        \n",
    "#         regr = MLPRegressor(activation=\"relu\", solver=\"sgd\", batch_size=160, learning_rate=\"adaptive\", learning_rate_init=0.002, power_t=0.1, momentum=0.9, max_iter=1000, random_state=2022)\n",
    "#         regr = xgb.XGBRegressor(learning_rate=0.1, max_depth=3, n_estimators=500)\n",
    "#         regr = Ridge(alpha=1.5)\n",
    "#         regr = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n",
    "        model_1 = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n",
    "                                   max_depth=4, max_features='sqrt',\n",
    "                                   min_samples_leaf=15,\n",
    "                                    min_samples_split=10, \n",
    "                                   loss='huber')\n",
    "    \n",
    "#         regr = make_pipeline(RobustScaler(),\n",
    "#                     GradientBoostingRegressor(n_estimators=5000, learning_rate=0.05,\n",
    "#                                    max_depth=4, max_features='sqrt',\n",
    "#                                    min_samples_leaf=15, min_samples_split=10, \n",
    "#                                    loss='huber', random_state =5))\n",
    "                            \n",
    "        \n",
    "#         model_2 = xgb.XGBRegressor(learning_rate=0.1, max_depth=3, n_estimators=500)\n",
    "#         model_2 = KNeighborsRegressor(n_neighbors=10, weights='distance')\n",
    "        \n",
    "#         kernel = 1.0**2 * Matern(length_scale=0.5, length_scale_bounds=(1e-05, 100000.0), nu=0.5)\n",
    "#         model_2 = GaussianProcessRegressor(kernel=kernel, alpha=5e-9, optimizer='fmin_l_bfgs_b', n_restarts_optimizer=0, normalize_y=False, copy_X_train=True)\n",
    "    \n",
    "#         model_2 = HistGradientBoostingRegressor(learning_rate=0.05,\n",
    "#                                    max_depth=4,\n",
    "#                                    min_samples_leaf=15)\n",
    "        \n",
    "        model_3 = MLPRegressor(solver=\"sgd\", learning_rate=\"adaptive\", learning_rate_init=0.002, max_iter=1000)\n",
    "        \n",
    "        eclf1 = VotingRegressor(estimators=[('gbr', model_1), ('mlp', model_3)])\n",
    "        eclf1 = eclf1.fit(X_train, y_train)\n",
    "        self.model = eclf1\n",
    "\n",
    "        return None\n",
    "\n",
    "    def predict(self, X_test: pd.DataFrame) -> np.array:\n",
    "        \"\"\"\n",
    "        Use the trained model to predict on un-seen dataset\n",
    "        You need to modify this function\n",
    "        :param X_test: shape (N, d), where N is the number of observations, d is feature dimension\n",
    "        return: prediction, shape (N,1)\n",
    "        \"\"\"\n",
    "        \n",
    "        X_test = X_test.drop(columns = self.to_drop)\n",
    "        y_pred = self.model.predict(X_test)\n",
    "#         y_pred_2 = self.model_2.predict(X_test)\n",
    "#         y_pred_3 = self.model_3.predict(X_test)\n",
    "        \n",
    "#         y_pred = (y_pred_1+y_pred_2+y_pred_3)/3.0\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bfa4bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# model = Model() # Model class imported from your submission\n",
    "X_train = pd.read_csv(\"/Users/williamlee/Desktop/Past Courses/CS599/class_challenge/data_cleaned_train_comments_X.csv\")  # pandas Dataframe\n",
    "y_train = pd.read_csv(\"/Users/williamlee/Desktop/Past Courses/CS599/class_challenge/data_cleaned_train_y.csv\")  # pandas Dataframe\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2)\n",
    "# model.train(X_train, y_train) # train your model on the dataset provided to you\n",
    "# y_pred = model.predict(X_test) # test your model on the hidden test set (pandas Dataframe)\n",
    "# mse = mean_squared_error(y_test, y_pred) # compute mean squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7e28bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# from sklearn.neural_network import MLPRegressor\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# import xgboost as xgb\n",
    "\n",
    "# # regr = MLPRegressor(hidden_layer_sizes = (100,), activation=\"relu\", solver=\"sgd\", batch_size=160, learning_rate=\"adaptive\", learning_rate_init=0.002, power_t=0.1, momentum=0.9, max_iter=1000)\n",
    "# # regr = MLPRegressor(hidden_layer_sizes = (100,), activation=\"relu\", solver=\"sgd\", learning_rate_init=0.002, max_iter=1000)\n",
    "# regr = xgb.XGBRegressor(objective='reg:squarederror', n_estimators = 400, learning_rate =0.1, random_state=2022)\n",
    "# regr.fit(X_train, y_train)\n",
    "# y_pred = regr.predict(X_test)\n",
    "\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "# print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a58830f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for x in [0.1, 0.2, 0.3]:\n",
    "# #     for y in [250, 300, 350]:\n",
    "# from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "# X_train = pd.read_csv(\"/Users/williamlee/Desktop/CS599/class_challenge/data_cleaned_train_comments_X.csv\")  # pandas Dataframe\n",
    "# y_train = pd.read_csv(\"/Users/williamlee/Desktop/CS599/class_challenge/data_cleaned_train_y.csv\")  # pandas Dataframe\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.33, random_state=2022)\n",
    "# cor_matrix = X_train.corr().abs()\n",
    "# upper_tri = cor_matrix.where(np.triu(np.ones(cor_matrix.shape),k=1).astype(np.bool))\n",
    "# to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.95)]\n",
    "# X_train = X_train.drop(columns = to_drop)\n",
    "\n",
    "# # feature_names = list(X_train.columns)\n",
    "# # for feature in feature_names:\n",
    "# #     upper_limit = X_train[feature].mean() + 3*X_train[feature].std()\n",
    "# #     lower_limit = X_train[feature].mean() - 3*X_train[feature].std()\n",
    "# #     new_X_train = X_train.copy()\n",
    "# #     new_X_train.loc[new_X_train[feature]>upper_limit, feature] = upper_limit\n",
    "# #     new_X_train.loc[new_X_train[feature]<lower_limit, feature] = lower_limit\n",
    "# #     X_train = new_X_train\n",
    "\n",
    "# y_train = np.array(y_train)\n",
    "# r,c = y_train.shape\n",
    "# y_train = y_train.reshape(r)\n",
    "\n",
    "\n",
    "# #         regr = MLPRegressor(activation=\"relu\", solver=\"sgd\", batch_size=160, learning_rate=\"adaptive\", learning_rate_init=0.002, power_t=0.1, momentum=0.9, max_iter=1000, random_state=2022)\n",
    "# # regr = xgb.XGBRegressor(objective='reg:squarederror', n_estimators = 400, learning_rate = 0.1, random_state=2022)\n",
    "# regr = Ridge(alpha=3, random_state=2022)\n",
    "# regr.fit(X_train, y_train)\n",
    "# X_test = X_test.drop(columns = to_drop)\n",
    "# y_pred = regr.predict(X_test)\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "# print(mse)\n",
    "        \n",
    "# #best 0.11982430715908494  350  0.1\n",
    "# #0.1452965260127538 1.0\n",
    "# #0.14528293249677898 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56f4f861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for x in [0.1, 0.2, 0.3]:\n",
    "# #     for y in [250, 300, 350]:\n",
    "\n",
    "\n",
    "# X_train = pd.read_csv(\"/Users/williamlee/Desktop/CS599/class_challenge/data_cleaned_train_comments_X.csv\")  # pandas Dataframe\n",
    "# y_train = pd.read_csv(\"/Users/williamlee/Desktop/CS599/class_challenge/data_cleaned_train_y.csv\")  # pandas Dataframe\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.33, random_state=2022)\n",
    "# cor_matrix = X_train.corr().abs()\n",
    "# upper_tri = cor_matrix.where(np.triu(np.ones(cor_matrix.shape),k=1).astype(np.bool))\n",
    "# to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.99)]\n",
    "# X_train = X_train.drop(columns = to_drop)\n",
    "\n",
    "# feature_names = list(X_train.columns)\n",
    "# for feature in feature_names:\n",
    "#     upper_limit = X_train[feature].mean() + 3*X_train[feature].std()\n",
    "#     lower_limit = X_train[feature].mean() - 3*X_train[feature].std()\n",
    "#     new_X_train = X_train.copy()\n",
    "#     new_X_train.loc[new_X_train[feature]>upper_limit, feature] = upper_limit\n",
    "#     new_X_train.loc[new_X_train[feature]<lower_limit, feature] = lower_limit\n",
    "#     X_train = new_X_train\n",
    "\n",
    "# y_train = np.array(y_train)\n",
    "# r,c = y_train.shape\n",
    "# y_train = y_train.reshape(r)\n",
    "\n",
    "\n",
    "# #         regr = MLPRegressor(activation=\"relu\", solver=\"sgd\", batch_size=160, learning_rate=\"adaptive\", learning_rate_init=0.002, power_t=0.1, momentum=0.9, max_iter=1000, random_state=2022)\n",
    "# regr = xgb.XGBRegressor(objective='reg:squarederror', n_estimators = y, learning_rate =x, random_state=2022)\n",
    "# regr.fit(X_train, y_train)\n",
    "# X_test = X_test.drop(columns = to_drop)\n",
    "# y_pred = regr.predict(X_test)\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "# print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "169201c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/30/l2nnp8jn4tv3n3lp4v0gjvhh0000gn/T/ipykernel_18634/2935421019.py:42: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  upper_tri = cor_matrix.where(np.triu(np.ones(cor_matrix.shape),k=1).astype(np.bool))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13627437584720034\n",
      "Duration: 0:03:07.515489\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from datetime import datetime\n",
    "start_time = datetime.now()\n",
    "\n",
    "\n",
    "model = Model()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.33)\n",
    "model.train(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(mse)\n",
    "# do your work here\n",
    "end_time = datetime.now()\n",
    "print('Duration: {}'.format(end_time - start_time))\n",
    "\n",
    "#0.12109658812758879 for learning rate = 0.05\n",
    "#0.129"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001404a3",
   "metadata": {
    "id": "001404a3"
   },
   "source": [
    "**GOOD LUCK!**\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
